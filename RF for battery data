#%% Data prep
Lith_Batt1 = pd.read_csv("sample1.csv")
print(Lith_Batt1.info())
print(Lith_Batt1.head())
df1 = Lith_Batt1

# Scale data
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(df1)



X1 = dataset[:, [1, 2, 3, 4, 5, 6]] # C, V, CC, DC, CE, DE
y1 = dataset[:, 7] # SOC

# Encode dataset to run regression
from sklearn import preprocessing
from sklearn import utils

lab_enc = preprocessing.LabelEncoder()
encoded = lab_enc.fit_transform(y1)

X_train, X_test, y_train, y_test = train_test_split(X1, encoded, test_size=0.2, random_state=17)

# Split data into train and test sets
train_set, test_set = train_test_split(Lith_Batt1, test_size=0.2)

#print('After scaling the sample looks like:', dataset.describe)
print('The size of the Dataset is:', dataset.shape)
print('The training set is:', X_train.shape)
print('And the test set is:', X_test.shape)

#%% Packages for RF
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor

#%% Making Feature list
features = Lith_Batt1.drop(columns=['Test Time (s)','dV/dt(V/s)'])
feature_list = list(features.columns)

#%% Random Forest for Regression
bag_reg = BaggingRegressor(
    DecisionTreeRegressor(splitter="random", max_leaf_nodes=16, random_state=17),
    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=17)

bag_reg.fit(X_train, y_train)
y_pred = bag_reg.predict(X_test)

rnd_reg = RandomForestRegressor(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=17)
rnd_reg.fit(X_train, y_train)

y_pred_rf = rnd_reg.predict(X_test)

importances = list(rnd_reg.feature_importances_)
print(importances)


# Visualize the Tree
from sklearn.tree import export_graphviz
import pydot
from IPython.display import Image

tree = rnd_reg.estimators_[6]
export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)

(graph, ) = pydot.graph_from_dot_file('tree.dot')
graph.write_png('tree.png')
Image(graph.create_png())

# Save Image
# PDF
graph.write_pdf('tree.pdf')

# Limit depth of tree to 3 levels
rf_limit = RandomForestRegressor(n_estimators=10, max_depth = 3)
rf_limit.fit(X_train, y_train)

# Extract Smaller Tree
small_tree = rf_limit.estimators_[6]

# Save smaller tree
export_graphviz(small_tree, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)
(graph, ) = pydot.graph_from_dot_file('small_tree.dot')
graph.write_png('small_tree.png')
graph.write_pdf('small_tree.pdf')
Image(graph.create_png())

# List of importances
importances = list(rnd_reg.feature_importances_)

feature_importances = [(feature, round(importance, 2)) for feature, importance 
                       in zip(feature_list, importances)]

feature_importances = sorted(feature_importances, key = lambda x: x[1], 
                             reverse = True)

[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in 
 feature_importances]

# Current and Voltage seem to be the most important
# New RF for the two most important features
rf_most_important = RandomForestRegressor(n_estimators= 1000, random_state=17)

# Extract two most important
important_indices = [feature_list.index('Current (A)'), feature_list.index('Voltage (V)')]
train_important = X_train[:, important_indices]
test_important = X_test[:, important_indices]

rf_most_important.fit(train_important, y_train)

predictions = rf_most_important.predict(test_important)

errors = abs(predictions - y_test)

# Print performance metrics
print('MAE is:', round(np.mean(errors), 2), 'degrees.')

mape = np.mean(100 * (errors / y_test))
accuracy = 100 - mape

print('Accuracy is:', round(accuracy, 2), '%.')

# Plotting Importances
# Set the style
plt.style.use('fivethirtyeight')
# list of x locations for plotting
x_values = list(range(len(importances)))
# Make a bar chart
plt.bar(x_values, importances, orientation = 'vertical')
# Tick labels for x axis
plt.xticks(x_values, feature_list, rotation='vertical')
# Axis labels and title
plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Feature Importances');
